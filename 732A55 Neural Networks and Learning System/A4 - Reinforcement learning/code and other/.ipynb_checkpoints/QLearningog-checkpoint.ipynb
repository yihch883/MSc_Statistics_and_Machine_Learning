{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Quick introduction to jupyter notebooks**\n",
    "* Each cell in this notebook contains either code or text.\n",
    "* You can run a cell by pressing Ctrl-Enter, or run and advance to the next cell with Shift-Enter.\n",
    "* Code cells will print their output, including images, below the cell. Running it again deletes the previous output, so be careful if you want to save some results.\n",
    "* You don't have to rerun all cells to test changes, just rerun the cell you have made changes to. Some exceptions might apply, for example if you overwrite variables from previous cells, but in general this will work.\n",
    "* If all else fails, use the \"Kernel\" menu and select \"Restart Kernel and Clear All Output\". You can also use this menu to run all cells.\n",
    "* A useful debug tool is the console. You can right-click anywhere in the notebook and select \"New console for notebook\". This opens a python console which shares the environment with the notebook, which let's you easily print variables or test commands.\n",
    "\n",
    "### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules when changed\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Plot figures \"inline\" with other output\n",
    "%matplotlib inline\n",
    "\n",
    "# Most important package\n",
    "import numpy as np\n",
    "\n",
    "# The reinforcement learning environment\n",
    "from gridworld import GridWorld\n",
    "\n",
    "# Configure nice figures\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.facecolor']='white'\n",
    "plt.rcParams['figure.figsize']=(14,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***! IMPORTANT NOTE !***\n",
    "\n",
    "Your implementation should only use the `numpy` (`np`) module. The `numpy` module provides all the functionality you need for this assignment and makes it easier debuging your code. No other modules, e.g. `scikit-learn` or `scipy` among others, are allowed and solutions using modules other than `numpy` will be sent for re-submission. You can find everything you need about `numpy` in the official [documentation](https://numpy.org/doc/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **1. Reinforcement Learning, introduction**\n",
    "In the previous assignments we have explored supervised learning, in other words, methods that train a model based on known inputs and targets. This time, we will instead look at a branch of machine learning that is much closer to the intuitive notion of \"learning\". Reinforcement learning, or RL for short, does not work with inputs and targets, but instead learns by performing **actions** in an **environment** and observing the generated **rewards**.\n",
    "\n",
    "RL is a very broad concept and many different algorithms have been deviced based on these general concepts of actions and rewards. Perhaps the main advantage of RL over other machine learning techniques is that we do not explicitly tell the model what the right answer is (like we have done in the previous assignments), but instead only tell the model when the desired outcome has been acheived. This might seem like the same thing at first, but the key difference is that RL allows the model to device solutions that outperform the human teacher. This is usually not possible in traditional supervised learning since the model can only get as good as the training data (the teacher). With the freedom to explore new strategies, which is inherent to RL, this is no longer true and some truly astounding results have been acheved. The most famous example is probably AlphaGo, the first computer program to beat a human expert in the board game Go. [Here is an excellent documentary](https://youtu.be/WXuK6gekU1Y), if you have some time to spare. For those of you that want a quicker and more fun example, [here is a video about RL agents playing hide and seek](https://youtu.be/kopoLzvh5jY), which very clearly demonstrates the power of RL to invent new and hidden strategies.\n",
    "\n",
    "Of course, these examples are from the very forefront of current research in RL, and are unfortunately too complex for this assignment. We will instead work on a much simpler problem, but the core concepts that you will implement and investigate here are the same that made the above possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1 Getting to know the environment interface**\n",
    "To do this assignment you must first get familiar with the code interface to the environment, or \"World\", as we will call it. You will work with a special type of environment called a **GridWorld**. The GridWorld is, as the name suggests, a world where each state is represented by a square on a grid. To create an instance of a GridWorld, run the following code. You can change the input number to select a different world. You will work with worlds 1-4, but there are other optional worlds as well, which we encourage you to explore at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = GridWorld(1)\n",
    "W.init()\n",
    "W.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 1:</span>**\n",
    "The colored background represents the reward for entering each state. Notice that all rewards are negative. Can you think of why this is important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answer here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Agent** is represented by the gray square, and will traverse the environment in order to reach the **goal** state, represented by the green circle.\n",
    "You can access all information you need regarding the state of the GridWorld by the methods of the World class. Here is the full list with explanations for each method:\n",
    "\n",
    "* `getWorldSize()` - Returns a tuple with the size of each dimension in the state space. For the GridWorlds, this is the y-size and x-size of the grid.\n",
    "* `getDimensionNames()` - Returns a list with the names for each dimension. This is only used to understand the world better, and should not be used to design the algorithm.\n",
    "* `getActions()` - Returns a list of available actions in the form of strings. These are the only accepted values to pass to `doAction`.\n",
    "* `init()` - Initializes the World. For example this resets the position of the agent in the GridWorlds. Do this at the beginning of each epoch.\n",
    "* `getState()` - Returns the current state of the World, which for a GridWorld is the position of the agent.\n",
    "* `doAction(act)` - Performs an action and returns a 2-tuple indicating if the actions was valid, and the corresponding reward.\n",
    "* `draw(epoch, Q)` - Update any plots associated with the World. The two arguments are optional but will include more information in the plots if you provide them.\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = GridWorld(1)\n",
    "print(\"World size:\", W.getWorldSize())\n",
    "print(\"Dimension names:\", W.getDimensionNames())\n",
    "print(\"Actions:\", W.getActions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of some actions in the first GridWorld. Read the code and output and make sure you understand how this works before proceeding. You can quickly run the cell multiple times by holding `Ctrl` and pressing `Enter` to generate a new output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = GridWorld(1)\n",
    "W.init()\n",
    "\n",
    "# Check state\n",
    "state, isTerm = W.getState()\n",
    "print(f\"State initialized to {state}.\")\n",
    "\n",
    "# Make action\n",
    "a = \"Down\"\n",
    "isValid, reward = W.doAction(a)\n",
    "print(f\"Action '{a}' was {'' if isValid else 'not '}valid and gave a reward of {reward}.\")\n",
    "\n",
    "# Check state\n",
    "state, isTerm = W.getState()\n",
    "print(f\"State is {state} and is {('' if isTerm else 'not ')}terminal.\")\n",
    "\n",
    "# Make action\n",
    "a = \"Right\"\n",
    "isValid, reward = W.doAction(a)\n",
    "print(f\"Action '{a}' was {'' if isValid else 'not '}valid and gave a reward of {reward}.\")\n",
    "\n",
    "# Check state\n",
    "state, isTerm = W.getState()\n",
    "print(f\"State is {state} and is {('' if isTerm else 'not ')}terminal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **2. Implementing the Q-learning algorithm**\n",
    "You will now implement the main algorithm of this assignment, **Q-learning**. This algorithm is powerful since it allows the simultaneous exploration of different **policies**. This is done by a state-action table **Q**, keeping track of the expected reward associated with each action in each state. By iteratively updating these estimates as we get new rewards, the policies explored by the agent eventually converges to the optimal policy. This can all be summarized in the following equation:\n",
    "\n",
    "$$ \\large Q\\left(s_t,a\\right) \\leftarrow \\underbrace{Q\\left(s_t,a\\right)}_{\\mathrm{Old \\space value}} \\cdot \\left(1-\\alpha\\right) + \\alpha \\cdot \\underbrace{\\left(r + \\gamma V\\left(s_{t+1}\\right)\\right)}_{\\mathrm{New \\space estimate}} $$\n",
    "\n",
    "This defines that the value of $Q$ in a state $s_t$ for action $a$, i.e $Q\\left(s_t,a\\right)$, should be updated as a weighted average of the old value and a new estimate, where the weighting is based on the learning rate $\\alpha \\in (0,1)$. The new estimate is a combination of the reward $r$ for the action we are updating, and the estimated value $V$ of the next state $s_{t+1}$, discounted by the factor $\\gamma \\in (0,1]$. By increasing $\\gamma$, the future value is weighted higher, which is why we say that this optimizes for long-term rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 The training function**\n",
    "First, you will implement the Q-learning algorithm training loop in the following function. The inputs to this function is a World object, and a dictionary for any parameters needed for the training. This dictionary will contain the following parameters, which you will need `params = {\"Epochs\": 100, \"MaxSteps\": 100: \"Alpha\": 0.5: \"Gamma\": 0.9, \"ExpRate\": 0.5, \"DrawInterval\": 100}`. Note that these values are only examples, you will have to change them when optimizing each world. You access the content of the dictionary by it's name, for example `params[\"Gamma\"]`. Using this style makes it very easy later in the notebook to try new worlds and parameter combinations.\n",
    "\n",
    "Finally before you begin, here are some concrete tips to keep in mind while working:\n",
    "* Try your code often! Jump ahead to section 3.1 to easily run the training in the first GridWorld.\n",
    "* As part of this implementation, you must also implement the functions `getpolicy` and `getvalue` in `utils.py`. When you have implemented these the `draw` function will automatically show the results of the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(World, params={}):\n",
    "    \n",
    "    # Init world and get size of dimensions\n",
    "    WSize = World.getWorldSize()\n",
    "    A = World.getActions()\n",
    "    NA = len(A)\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    # Initialize the Q-matrix (use the size variables above)\n",
    "    Q = ???\n",
    "    \n",
    "    for i in range(params[\"Epochs\"]):\n",
    "        ???\n",
    "        \n",
    "        # Limiting the number of steps in an epoch prevents getting stuck in infinite loops\n",
    "        for j in range(params[\"MaxSteps\"]):\n",
    "            ???\n",
    "        \n",
    "        # Update plots with regular intervals\n",
    "        if ((i+1) % params[\"DrawInterval\"] == 0) or (i == params[\"Epochs\"]-1):\n",
    "            World.draw(epoch=(i+1), Q=Q)\n",
    "    \n",
    "    # ============================================\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 The test function**\n",
    "It's important to test the performance of the trained model. This *could* be done with some heuristic function that measures properties such as path lenghts and total rewards, but here we choose to instead use a more direct evaluation method. In the following function you should implement a test loop where you follow the optimal policy and draw the world after *each* action. Since this is code to test the trained model, you should not update Q, only use it to determine the optimal actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearningTest(W, Q, params={}):\n",
    "    \n",
    "    # The number of epochs is now the number of tests runs to do\n",
    "    for i in range(params[\"Epochs\"]):\n",
    "        \n",
    "        # Init the world and get state\n",
    "        W.init()\n",
    "        A = W.getActions()\n",
    "        s,_ = W.getState()\n",
    "\n",
    "        # Again we limit the number of steps to prevent infinite loops\n",
    "        for j in range(params[\"MaxSteps\"]):\n",
    "            \n",
    "            # --------------------------------------------\n",
    "            # === Your code here =========================\n",
    "            # --------------------------------------------\n",
    "            \n",
    "            # Choose and perform optimal action from policy\n",
    "            ???\n",
    "\n",
    "            # ============================================\n",
    "            \n",
    "            # Get updated state and draw\n",
    "            s,isTerm = W.getState()\n",
    "            W.draw(epoch=(i+1), Q=Q)\n",
    "            \n",
    "            # Check if goal\n",
    "            if isTerm:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **3. Optimizing the different worlds**\n",
    "\n",
    "In this section you will optimize the hyperparameters to train the 4 first GridWorlds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 GridWorld 1**\n",
    "We start with the simplest of the worlds, \"Annoying block\". The policy should converge without much difficulty, so use this as a test to see if your implementaion is correct. If you use a good set of hyperparameters, you can expect a rather neat policy in about 1000 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = GridWorld(1)\n",
    "\n",
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "Q1 = QLearning(W1, params={\"LR\": ???, \"Gamma\": ???, \"Eps\": ???, \"Epochs\": ???, \"MaxSteps\": 200, \"DrawInterval\": 100})\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to run a few tests with the optimized policy to see if the solution looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QLearningTest(W=W1, Q=Q1, params={\"Epochs\": 5, \"MaxSteps\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 2:</span>**\n",
    "1. Describe World 1.\n",
    "2. What is the goal for the agent in this world?\n",
    "3. What is a good choice of learning rate in this world? Motvate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now continue optimizing worlds 2-4. Note that the optimal hyperparmeters potentially are very different for each world.\n",
    "\n",
    "### **3.2 GridWorld 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = GridWorld(2)\n",
    "\n",
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "Q2 = QLearning(W2, params={\"LR\": ???, \"Gamma\": ???, \"Eps\": ???, \"Epochs\": ???, \"MaxSteps\": 200, \"DrawInterval\": 100})\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to run a few tests with the optimized policy to see if the solution looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QLearningTest(W=W2, Q=Q2, params={\"Epochs\": 5, \"MaxSteps\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 3:</span>**\n",
    "1. Describe World 2.\n",
    "2. This world has a hidden trick. Describe this trick and why this can be solved with reinforcement learning.\n",
    "3. What is the goal for the agent in this world?\n",
    "4. What is a good choice of learning rate in this world? Motvate your answer.\n",
    "5. Compared to the optimal policy in World 1, how do we expect the optimal policy to look in this world? Motivate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 GridWorld 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W3 = GridWorld(3)\n",
    "\n",
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "Q3 = QLearning(W3, params={\"LR\": ???, \"Gamma\": ???, \"Eps\": ???, \"Epochs\": ???, \"MaxSteps\": 200, \"DrawInterval\": 100})\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to run a few tests with the optimized policy to see if the solution looks reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QLearningTest(W=W3, Q=Q3, params={\"Epochs\": 5, \"MaxSteps\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 4:</span>**\n",
    "1. Describe World 3.\n",
    "2. From the perspective of the learning algorithm, how does this world compare to World 1?\n",
    "3. What is the goal for the agent in this world?\n",
    "4. Is it possible to get a good policy in every state in this world? If so, which hyperparameter is particulary important to acheive this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.4 GridWorld 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W4 = GridWorld(4)\n",
    "\n",
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "Q4 = QLearning(W4, params={\"LR\": ???, \"Gamma\": ???, \"Eps\": ???, \"Epochs\": ???, \"MaxSteps\": 200, \"DrawInterval\": 100})\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to run a few tests with the optimized policy to see if the solution looks reasonable. **Important**: You might think the policy above looks bad, but we encourage you to run this test even if you think it's not optimal. It might give you some insight into the world behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QLearningTest(W=W4, Q=Q4, params={\"Epochs\": 5, \"MaxSteps\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 5:</span>**\n",
    "1. Describe World 4 using your own words. \n",
    "2. This world has a hidden trick. What is it, and how does this world differ from World 3?\n",
    "3. What is the goal for the agent in this world?\n",
    "4. What is a good choice of learning rate in this world? Motvate your answer.\n",
    "5. How should we expect the optimal policy too look like? In other words, what is the optimal path from start to goal in this world? Motivate your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **4. Investigating the effects of hyperparameters**\n",
    "You will now design a series of experiments to show the impact of the three main hyperparameters - learning rate, discount factor, and exploration rate - in different environments. You are free to extend the experiments as you see fit in order to make your point in the discussions, but a recommended strategy is to try two extreme cases (low vs high values). For each parameter, there is one world in particular of the four you have already used where it is easy to show the effects we are looking for. Figuring out which worlds is part of the excercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "W_LR = GridWorld(???)\n",
    "Q_41_L = QLearning(W_LR, params={\"LR\": ???, \"Gamma\": ???, \"Eps\": ???, \"Epochs\": ???, \"MaxSteps\": 200, \"DrawInterval\": 100})\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "Q_LR_H = QLearning(W_LR, params={\"LR\": ???, \"Gamma\": ???, \"Eps\": ???, \"Epochs\": ???, \"MaxSteps\": 200, \"DrawInterval\": 100})\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 6:</span>**\n",
    "Explain your experiment and results, and why you choose this world (your answers should be based on the output of the cells above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answer here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Discount factor (gamma)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "W_DF = GridWorld(???)\n",
    "Q_DF_L = QLearning(W_DF, params={\"LR\": ???, \"Gamma\": ???, \"Eps\": ???, \"Epochs\": ???, \"MaxSteps\": 200, \"DrawInterval\": 100})\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "Q_DF_H = QLearning(W_DF, params={\"LR\": ???, \"Gamma\": ???, \"Eps\": ???, \"Epochs\": ???, \"MaxSteps\": 200, \"DrawInterval\": 100})\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 7:</span>**\n",
    "Explain your experiment and results, and why you choose this world (your answers should be based on the output of the cells above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answer here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3 Exploration rate (epsilon)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "W_ER = GridWorld(???)\n",
    "Q_ER_L = QLearning(W_ER, params={\"LR\": ???, \"Gamma\": ???, \"Eps\": ???, \"Epochs\": ???, \"MaxSteps\": 200, \"DrawInterval\": 100})\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "Q_ER_H = QLearning(W_ER, params={\"LR\": ???, \"Gamma\": ???, \"Eps\": ???, \"Epochs\": ???, \"MaxSteps\": 200, \"DrawInterval\": 100})\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 8:</span>**\n",
    "Explain your experiment and results, and why you choose this world (your answers should be based on the output of the cells above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\\[ Your answer here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **5. Optional worlds**\n",
    "\n",
    "You have now investigated the four most important GridWorlds in the lab, but we have also created some optional worlds (numbers 5 to 7) which you can try to solve. There is also World 8, but that is a special case, so scroll down a few cells if you are interested. Here is a brief description of World 5 to 7:\n",
    "- World 5, Warpspace: As the name suggests, in this world there is one tile in which the agent enters warpspace and imediatly moves to another specific location. How do you think this will affect the learning?\n",
    "- World 6, Torus: In this world, the opposite edges are connected together like a rolled-up paper. If you connect both the up-down and left-right edges, you get a mathematical shape called a torus which has no edges. This means that the closest path to the goal might not be obvious anymore.\n",
    "- World 7, Steps: This world is a staircase of increasing rewards (although still all negative). However, moving up the stairs towards higher rewards also puts the agent further from the goal. So what is the optimal choice, to go for the long path with higher rewards, or to sprint throught the low rewards towards the goal. This depends on the value of gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WOpt = GridWorld( ??? )\n",
    "QOpt = QLearning(WOpt, {\"LR\": 0.99, \"Gamma\": 0.9, \"Eps\": 0.9, \"Epochs\": 1000, \"MaxSteps\": 200, \"DrawInterval\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QLearningTest(W=WOpt, Q=QOpt, params={\"Epochs\": 5, \"MaxSteps\": 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1: World 8**\n",
    "\n",
    "So far, every world has been a 2D-grid (y and x dimensions), and the four actions have been the same in every world. It has therefore been possible to write the code with this in mind, probably resulting in code where you index Q for example with `Q[s[0], s[1], a]` for a given state `s` and action `a`. However, it is possible to slightly rewrite the code to be independent of the number of dimensions in the state space, which means that we can then explore much more interesting worlds. It is also a nice excercise in how to write code that is general and modular. The way to do this is to index Q in the following way: `Q[(*s,a)]`. It's perfectly fine if you want to consider this as \"python magic\", but for the interested here is an explaination.\n",
    "\n",
    "The state `s` is a tuple, for example `(3,6)`. A quirk in python is that tuples can be used to index into arrays, with each value in the tuple indexing separate dimensions in the array. For example, if Q is a 10x15x4 array, then `Q[(3,6)]` will return the vector of four values in Q that are in the 3rd row and 6th column (i.e. all the action values for state `s = (3,6)`). The problem is that we want to access the Q-value of a specific action when updating with a new reward. One might assume that `Q[s,a]` would work, but this now works differently since we explicitly index Q with not only a tuple. The solution is to remake a tuple that contains both `s` and `a`, and then index Q with this. We can do this by first unpacking the state tuple by calling `*s`, then creating a new tuple with `(*s,a)`, containing both the state and action. For example, if `s = (3,6)` and `a = 2`, then `(*s,a) = (3,6,2)`. We then use this tuple to index into Q as `Q[(*s,a)]`.\n",
    "\n",
    "With this change to the implementation, we can for example extend the world to a 3D-grid, and your code should work the same. Let's try it in World 8, where the agent has the choice of moving between two floors of the map. This is shown as diagonal up or diagonal down arrows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W8 = GridWorld(8)\n",
    "Q8 = QLearning(W8, {\"LR\": 0.99, \"Gamma\": 0.9, \"Eps\": 0.9, \"Epochs\": 1000, \"MaxSteps\": 200, \"DrawInterval\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QLearningTest(W=W8, Q=Q8, params={\"Epochs\": 5, \"MaxSteps\": 100})"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd50aace418a96e8a4fe691a4d2292bd7058ca4eeebcf0b6e2084f539c4e7b28"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
